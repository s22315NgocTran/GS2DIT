{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s22315NgocTran/GS2DIT/blob/main/gpt_2_shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "8a248a4d-c5ac-4a02-c466-eea639906e1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.10)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.32.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "0b092916-c801-4327-d83f-323b26cad22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 683Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 5.11Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 436Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:10, 45.7Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 788Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 6.39Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 6.85Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "1a2873c9-2e06-484b-c7c0-1f4f6f809749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "41978740-7547-4658-c0df-6a03468eea0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth?\n",
            "\n",
            "I don't know. I don't think I can understand that. I mean, I'm not saying it's a planet, but it's a planet with a planet. At the end of the day, we don't know what happened\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "51f9ff97-287d-4c94-ab61-7c27b14fb7d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 15:19:03--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.95.189, 54.231.169.144, 52.217.50.6, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.95.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txtâ€™\n",
            "\n",
            "nietzsche.txt       100%[===================>] 586.82K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-03-21 15:19:03 (4.21 MB/s) - â€˜nietzsche.txtâ€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "8293fd51-f5d8-4347-86b4-e8faaa8f7692",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 161MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "b59755af-4d8a-4592-c71a-bee603432b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 15:19:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-03-21 15:19:13 (19.9 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'got1.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "a82aaace-f4a5-4a38-a50d-4e1e37ce701e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 433157 tokens\n",
            "Training...\n",
            "[1 | 7.27] loss=3.39 avg=3.39\n",
            "[2 | 9.30] loss=3.42 avg=3.41\n",
            "[3 | 11.33] loss=3.31 avg=3.37\n",
            "[4 | 13.37] loss=3.42 avg=3.39\n",
            "[5 | 15.41] loss=3.51 avg=3.41\n",
            "[6 | 17.46] loss=3.44 avg=3.42\n",
            "[7 | 19.50] loss=3.31 avg=3.40\n",
            "[8 | 21.55] loss=3.22 avg=3.38\n",
            "[9 | 23.60] loss=3.30 avg=3.37\n",
            "[10 | 25.65] loss=3.17 avg=3.35\n",
            "[11 | 27.72] loss=3.09 avg=3.32\n",
            "[12 | 29.77] loss=3.21 avg=3.31\n",
            "[13 | 31.82] loss=3.26 avg=3.31\n",
            "[14 | 33.89] loss=2.98 avg=3.28\n",
            "[15 | 35.96] loss=3.20 avg=3.28\n",
            "[16 | 38.03] loss=3.11 avg=3.27\n",
            "[17 | 40.15] loss=3.11 avg=3.26\n",
            "[18 | 42.22] loss=3.16 avg=3.25\n",
            "[19 | 44.29] loss=3.32 avg=3.26\n",
            "[20 | 46.36] loss=3.07 avg=3.25\n",
            "[21 | 48.44] loss=3.28 avg=3.25\n",
            "[22 | 50.52] loss=3.18 avg=3.24\n",
            "[23 | 52.61] loss=3.10 avg=3.24\n",
            "[24 | 54.69] loss=3.15 avg=3.23\n",
            "[25 | 56.78] loss=3.13 avg=3.23\n",
            "[26 | 58.86] loss=3.04 avg=3.22\n",
            "[27 | 60.95] loss=3.03 avg=3.21\n",
            "[28 | 63.06] loss=3.11 avg=3.21\n",
            "[29 | 65.17] loss=3.03 avg=3.20\n",
            "[30 | 67.27] loss=3.07 avg=3.20\n",
            "[31 | 69.37] loss=3.06 avg=3.19\n",
            "[32 | 71.48] loss=3.19 avg=3.19\n",
            "[33 | 73.59] loss=2.91 avg=3.18\n",
            "[34 | 75.72] loss=2.87 avg=3.17\n",
            "[35 | 77.85] loss=3.06 avg=3.17\n",
            "[36 | 79.98] loss=2.93 avg=3.16\n",
            "[37 | 82.13] loss=3.10 avg=3.16\n",
            "[38 | 84.27] loss=3.06 avg=3.15\n",
            "[39 | 86.42] loss=3.05 avg=3.15\n",
            "[40 | 88.57] loss=3.02 avg=3.15\n",
            "[41 | 90.73] loss=3.03 avg=3.14\n",
            "[42 | 92.88] loss=2.96 avg=3.14\n",
            "[43 | 95.04] loss=2.98 avg=3.13\n",
            "[44 | 97.22] loss=2.87 avg=3.13\n",
            "[45 | 99.38] loss=3.21 avg=3.13\n",
            "[46 | 101.56] loss=2.87 avg=3.12\n",
            "[47 | 103.73] loss=3.01 avg=3.12\n",
            "[48 | 105.91] loss=2.89 avg=3.11\n",
            "[49 | 108.09] loss=3.04 avg=3.11\n",
            "[50 | 110.28] loss=3.04 avg=3.11\n",
            "[51 | 112.48] loss=2.89 avg=3.10\n",
            "[52 | 114.68] loss=2.96 avg=3.10\n",
            "[53 | 116.89] loss=2.78 avg=3.09\n",
            "[54 | 119.10] loss=3.00 avg=3.09\n",
            "[55 | 121.32] loss=3.10 avg=3.09\n",
            "[56 | 123.54] loss=2.91 avg=3.09\n",
            "[57 | 125.78] loss=2.80 avg=3.08\n",
            "[58 | 128.02] loss=3.05 avg=3.08\n",
            "[59 | 130.26] loss=2.84 avg=3.07\n",
            "[60 | 132.51] loss=2.89 avg=3.07\n",
            "[61 | 134.76] loss=3.01 avg=3.07\n",
            "[62 | 137.02] loss=2.81 avg=3.06\n",
            "[63 | 139.28] loss=2.72 avg=3.05\n",
            "[64 | 141.53] loss=2.90 avg=3.05\n",
            "[65 | 143.77] loss=3.06 avg=3.05\n",
            "[66 | 146.02] loss=2.99 avg=3.05\n",
            "[67 | 148.26] loss=2.97 avg=3.05\n",
            "[68 | 150.49] loss=2.92 avg=3.05\n",
            "[69 | 152.72] loss=2.93 avg=3.04\n",
            "[70 | 154.93] loss=2.93 avg=3.04\n",
            "[71 | 157.16] loss=2.79 avg=3.04\n",
            "[72 | 159.37] loss=2.98 avg=3.04\n",
            "[73 | 161.59] loss=2.90 avg=3.03\n",
            "[74 | 163.80] loss=2.96 avg=3.03\n",
            "[75 | 166.01] loss=2.73 avg=3.03\n",
            "[76 | 168.22] loss=2.83 avg=3.02\n",
            "[77 | 170.42] loss=2.96 avg=3.02\n",
            "[78 | 172.63] loss=3.02 avg=3.02\n",
            "[79 | 174.84] loss=3.00 avg=3.02\n",
            "[80 | 177.04] loss=2.86 avg=3.02\n",
            "[81 | 179.24] loss=2.87 avg=3.02\n",
            "[82 | 181.45] loss=2.77 avg=3.01\n",
            "[83 | 183.65] loss=2.91 avg=3.01\n",
            "[84 | 185.86] loss=2.95 avg=3.01\n",
            "[85 | 188.06] loss=3.00 avg=3.01\n",
            "[86 | 190.26] loss=2.90 avg=3.01\n",
            "[87 | 192.47] loss=2.90 avg=3.00\n",
            "[88 | 194.67] loss=2.72 avg=3.00\n",
            "[89 | 196.88] loss=2.94 avg=3.00\n",
            "[90 | 199.10] loss=2.58 avg=2.99\n",
            "[91 | 201.31] loss=2.81 avg=2.99\n",
            "[92 | 203.52] loss=2.81 avg=2.99\n",
            "[93 | 205.73] loss=2.88 avg=2.98\n",
            "[94 | 207.95] loss=2.78 avg=2.98\n",
            "[95 | 210.18] loss=2.76 avg=2.98\n",
            "[96 | 212.41] loss=2.89 avg=2.98\n",
            "[97 | 214.63] loss=2.86 avg=2.97\n",
            "[98 | 216.86] loss=2.88 avg=2.97\n",
            "[99 | 219.08] loss=3.00 avg=2.97\n",
            "[100 | 221.31] loss=2.70 avg=2.97\n",
            "======== SAMPLE 1 ========\n",
            " sword sword is bound in a long piece of silk, and if his strength is not sufficient he rushes the field, or mounts a horse as a rider mounts a mountain. \n",
            "When he has run, or ridden with his son, or ridden with his son's brother, or ridden with his son's brother's nephew, these things happen. He \n",
            "can see no harm in these victories, for fear of his own pride. Only the king shall say, 'We rode well. You rode well.' If there is a man of honor in your \n",
            "lordship, and it seems to him a good thing, the Lannisters are your people, right or wrong. He can call this a manly \n",
            "woman's pride, and it is no wonder that some men quarrel more with one another than others. \n",
            "They laugh at the queen as far as the eye can see, and they shout at her when her name does not fit.\" \n",
            "\"Oh!\" A shadow of a smile crossed the hall, and there it would seem, for the first time in their whole lives. The \n",
            "whistle of laughter rang through the halls at the sudden sound of the wolf's throat, only a few words being heard across the hall. The king was \n",
            "reminding the queen that the word was an old custom, and that as she saw a man of honor standing before \n",
            "the door, holding a banner and saying \"no,\" it was a different story. \"Are you afraid?\" Jon asked Snow as the \n",
            "shadow of fear crossed the room. His eyes narrowed. \"I'm afraid! My brother is angry. He's my king, not my \n",
            "lord.\" \n",
            "\"You'll see?\" Lannister Jory gave him a wry smile. \"I'll see them all day.\" \n",
            "Then Snow followed, and a shadow of a smile crossed his face. \"Why would he?\" \n",
            "\"He's not my son, I suppose. He's always been. I have not seen him in his youth. It would be cruel to \n",
            "send him from Winterfell to the lands he promised them at the end of my reign. I'd give my son an \n",
            "out-of-town son if you would. Father does best, though, and if you would, I bet he would ride with your \n",
            "children. I would. I could send the Stark to seize those castles and seize the castle at Castle Black. Lord \n",
            "Targaryll would have the right to them. They'd have a thousand swords too. They may live to be old, and they might never \n",
            "take the kingship they promised us, but it's the right of Lord Tywin to seize the castle.\" \n",
            "Tyrion Lannister was the first man to speak to Ned Stark as his men escorted off to bed. The others shared in \n",
            "his surprise. Ned knew that Tyrion would be king shortly, at last. \n",
            "The king's office was guarded by a hundred men, and all were old men who had died with Lord Arryn in his \n",
            "womb. Tyrion was older, a younger one, for one thing. He saw that Ned could sense the difference between the child of a \n",
            "born lord and a lord heir, between the eldest son and the second son, and they were as a thousand \n",
            "brothers to the king. He had never known how old it felt to be king. \"They said you'd not take that title,\" \n",
            "he said. \n",
            "\"They said they're still waiting on you, Edmure,\" Ned confessed. \"I am still a boy. I know it, Lord Arryn. The King \n",
            "Page 88\n",
            "\n",
            "is dead. So I have not left my home yet, Tyrion. I will go with the queen. I may have \n",
            "one hour or so with Lord Arryn, but when I get to him, I cannot wait.\" \n",
            "No, Ned thought. \"I am coming to make him queen.\" Tyrion Lannister would be king, no doubt \n",
            "but he might not be as cruel as Lord Arryn himself. And not if your father does not love you. \n",
            "\"Not unless he wishes,\" Ned promised. \"Is that the way you say it?\" The king took a deep breath, looked at \n",
            "himself. \n",
            "As Tyrion Lannister stood tall in a Lannister gown of silk and blue silk, he bowed, and his king sat in the \n",
            "hall and drew the sword from a deep pocket on the table. \"A king is born free, Ned. You and your children will \n",
            "not be taken from a king, but you shall have them free. You shall marry them and call all men to your bedchamber and \n",
            "leave them and leave the rest to you as servants. Do that. If any man does not want an heir, he is no more than a man to \n",
            "keep his wife.\" \n",
            "\n",
            "\n",
            "[101 | 236.23] loss=2.88 avg=2.97\n",
            "[102 | 238.45] loss=2.77 avg=2.96\n",
            "[103 | 240.66] loss=2.87 avg=2.96\n",
            "[104 | 242.88] loss=2.70 avg=2.96\n",
            "[105 | 245.10] loss=2.81 avg=2.96\n",
            "[106 | 247.33] loss=2.79 avg=2.95\n",
            "[107 | 249.55] loss=2.79 avg=2.95\n",
            "[108 | 251.77] loss=2.87 avg=2.95\n",
            "[109 | 253.99] loss=2.79 avg=2.95\n",
            "[110 | 256.21] loss=2.76 avg=2.94\n",
            "[111 | 258.44] loss=2.78 avg=2.94\n",
            "[112 | 260.66] loss=2.85 avg=2.94\n",
            "[113 | 262.89] loss=2.68 avg=2.94\n",
            "[114 | 265.11] loss=2.89 avg=2.94\n",
            "[115 | 267.34] loss=2.79 avg=2.93\n",
            "[116 | 269.57] loss=2.88 avg=2.93\n",
            "[117 | 271.79] loss=2.75 avg=2.93\n",
            "[118 | 274.04] loss=2.92 avg=2.93\n",
            "[119 | 276.29] loss=2.69 avg=2.93\n",
            "[120 | 278.52] loss=2.77 avg=2.92\n",
            "[121 | 280.77] loss=2.81 avg=2.92\n",
            "[122 | 283.04] loss=2.92 avg=2.92\n",
            "[123 | 285.28] loss=2.82 avg=2.92\n",
            "[124 | 287.49] loss=2.78 avg=2.92\n",
            "[125 | 289.71] loss=2.89 avg=2.92\n",
            "[126 | 291.93] loss=2.80 avg=2.92\n",
            "[127 | 294.16] loss=2.79 avg=2.92\n",
            "[128 | 296.38] loss=2.81 avg=2.91\n",
            "[129 | 298.60] loss=2.65 avg=2.91\n",
            "[130 | 300.82] loss=2.87 avg=2.91\n",
            "[131 | 303.05] loss=2.71 avg=2.91\n",
            "[132 | 305.26] loss=2.75 avg=2.91\n",
            "[133 | 307.49] loss=2.81 avg=2.90\n",
            "[134 | 309.71] loss=2.73 avg=2.90\n",
            "[135 | 311.93] loss=2.87 avg=2.90\n",
            "[136 | 314.15] loss=2.71 avg=2.90\n",
            "[137 | 316.37] loss=2.58 avg=2.89\n",
            "[138 | 318.60] loss=2.74 avg=2.89\n",
            "[139 | 320.83] loss=2.80 avg=2.89\n",
            "[140 | 323.05] loss=2.77 avg=2.89\n",
            "[141 | 325.27] loss=2.65 avg=2.89\n",
            "[142 | 327.50] loss=2.76 avg=2.88\n",
            "[143 | 329.72] loss=2.65 avg=2.88\n",
            "[144 | 331.95] loss=2.59 avg=2.88\n",
            "[145 | 334.18] loss=2.74 avg=2.88\n",
            "[146 | 336.40] loss=2.77 avg=2.87\n",
            "[147 | 338.62] loss=2.80 avg=2.87\n",
            "[148 | 340.84] loss=2.61 avg=2.87\n",
            "[149 | 343.07] loss=2.61 avg=2.87\n",
            "[150 | 345.29] loss=2.75 avg=2.87\n",
            "[151 | 347.51] loss=2.71 avg=2.86\n",
            "[152 | 349.73] loss=2.69 avg=2.86\n",
            "[153 | 351.95] loss=2.76 avg=2.86\n",
            "[154 | 354.17] loss=2.80 avg=2.86\n",
            "[155 | 356.40] loss=2.86 avg=2.86\n",
            "[156 | 358.62] loss=2.80 avg=2.86\n",
            "[157 | 360.84] loss=2.71 avg=2.86\n",
            "[158 | 363.06] loss=2.59 avg=2.85\n",
            "[159 | 365.28] loss=2.64 avg=2.85\n",
            "[160 | 367.50] loss=2.63 avg=2.85\n",
            "[161 | 369.72] loss=2.63 avg=2.85\n",
            "[162 | 371.94] loss=2.57 avg=2.84\n",
            "[163 | 374.16] loss=2.71 avg=2.84\n",
            "[164 | 376.38] loss=2.92 avg=2.84\n",
            "[165 | 378.60] loss=2.68 avg=2.84\n",
            "[166 | 380.83] loss=2.59 avg=2.84\n",
            "[167 | 383.04] loss=2.72 avg=2.83\n",
            "[168 | 385.26] loss=2.65 avg=2.83\n",
            "[169 | 387.48] loss=2.47 avg=2.83\n",
            "[170 | 389.70] loss=2.77 avg=2.83\n",
            "[171 | 391.93] loss=2.49 avg=2.82\n",
            "[172 | 394.15] loss=2.57 avg=2.82\n",
            "[173 | 396.36] loss=2.85 avg=2.82\n",
            "[174 | 398.58] loss=2.58 avg=2.82\n",
            "[175 | 400.80] loss=2.71 avg=2.82\n",
            "[176 | 403.03] loss=2.58 avg=2.81\n",
            "[177 | 405.26] loss=2.56 avg=2.81\n",
            "[178 | 407.48] loss=2.65 avg=2.81\n",
            "[179 | 409.69] loss=2.62 avg=2.81\n",
            "[180 | 411.91] loss=2.60 avg=2.80\n",
            "[181 | 414.14] loss=2.50 avg=2.80\n",
            "[182 | 416.36] loss=2.57 avg=2.80\n",
            "[183 | 418.59] loss=2.60 avg=2.79\n",
            "[184 | 420.81] loss=2.58 avg=2.79\n",
            "[185 | 423.03] loss=2.89 avg=2.79\n",
            "[186 | 425.25] loss=2.65 avg=2.79\n",
            "[187 | 427.48] loss=2.55 avg=2.79\n",
            "[188 | 429.70] loss=2.52 avg=2.79\n",
            "[189 | 431.92] loss=2.50 avg=2.78\n",
            "[190 | 434.13] loss=2.61 avg=2.78\n",
            "[191 | 436.36] loss=2.68 avg=2.78\n",
            "[192 | 438.58] loss=2.76 avg=2.78\n",
            "[193 | 440.81] loss=2.66 avg=2.78\n",
            "[194 | 443.02] loss=2.71 avg=2.78\n",
            "[195 | 445.24] loss=2.73 avg=2.78\n",
            "[196 | 447.46] loss=2.57 avg=2.77\n",
            "[197 | 449.68] loss=2.60 avg=2.77\n",
            "[198 | 451.91] loss=2.56 avg=2.77\n",
            "[199 | 454.13] loss=2.75 avg=2.77\n",
            "[200 | 456.35] loss=2.44 avg=2.77\n",
            "======== SAMPLE 1 ========\n",
            " dead. That is when the battle began. \n",
            "Catelyn thought back to her days in the crypt. She could sense the pain in the man's eyes; a light as if he had been \n",
            "shaking, a sudden coldness that must have been painful. She would not have believed what some would tell a child, it \n",
            "was all there, on her scalp, deep around her ears and in her chest, black as blood. The darkness \n",
            "rushed through the place, around her face and inside her. She could find nothing to say, not a word. \n",
            "She remembered Catelyn's voice, she thought. \n",
            "\"You have to hurry with your children,\" Ser Jorah said. \"They have come from the Mother against us!\" \n",
            "\"Let us leave them.\" He stood up, so he did, but before she could even react, it was dead silence. \n",
            "The pain was soothed by the dust and the tears in her hair. She lifted her eyes to the floor where \n",
            "Theon Greyjoy had stood behind her, with Jotho guarding her when Joffrey had gone with his \n",
            "guard. When she saw her father slumped face against the floor, Catelyn realized what was happening. \n",
            "Inside Dany's Tower, on the far end of the crypt, she saw a light column of stone rising from the rock \n",
            "below with white fire in its lantern. The first arrow struck her, striking her. Catelyn looked up. \n",
            "\"Don't fight the arrow,\" she said in Dothraki, \"you have to fight the arrow.\" \n",
            "Catelyn felt something hot on her hand. She looked at Ser Jorah and Jhiqui who had been standing \n",
            "behind her in the crypt. Ser Jorah was leaning over a ledge, and she could not help but feel the fury in his eyes. She \n",
            "could not let go of him. \n",
            "\"You must be strong,\" Arya said. \n",
            "Page 282\n",
            "\n",
            "\"You must be strong, or I will kill you,\" Ser Jorah said. \n",
            "Catelyn took a deep breath, took one step forward and went toward her father. He was shaking, she knew, but it \n",
            "was not going to go away. \n",
            "\"You must leave the city and find a way to fetch your horses,\" she reminded him, \"or I will slay you.\" \n",
            "He laughed and turned to the others, and the horsemen gave chase. Ser Jorah went after him again. \"How is it that \n",
            "some of the clansmen say you will lead your people to the sword?\" \n",
            "\"The King's Hand,\" Jon Snow said uncertainly, \"but we will not be marching for the Dothraki's khalasar. I am \n",
            "speaking at Khal Drogo, and it is no secret I am loyal to him.\" He rose to his feet and turned, and Jon and Catelyn went \n",
            "from him. They descended the path that led up to the bridge to the Twins. Catelyn looked as though she would \n",
            "never understand if anyone interrupted her, and she tried to get them to listen for her pleas. They would always come \n",
            "Page 283\n",
            "\n",
            "for her, she realized, and she was right. The place was full of bloodshed, with axes and arrows. The only way to \n",
            "behead one was with the blade of a dragon. Once she knew the place, the next few hours she waited \n",
            "before her horse to ride to the Twins. The riverlands and the great plains of Jhiqui were lined with archers, \n",
            "and a strong wind blew out from the south as far as she could see, driving arrowheads upward and south. It was the \n",
            "great slaughter. As the road turned toward the city, horses were blown off their stables and carts were \n",
            "pulled up and loaded down with flour from the Dothraki market. It made for the most dangerous \n",
            "spot on the road, it seemed. Catelyn had come to her senses in a heartbeat; Arya had made herself \n",
            "strong, and she needed to learn not to let an enemy take the sword. \n",
            "She rode past a river of women and children and men and horses and goats. Her ears began to ache, \n",
            "and the smell caught Catelyn's breath. She ran down the steps and into the street. She was walking in what must have been \n",
            "the right direction, not a left, but at this point she could hardly believe it. If she were to stand still while Catelyn \n",
            "slash at them, she might choke one of them and die. A quick look down the street went a certain rage. \n",
            "She felt the leather beneath her boots turn to stone, the sweat filling her hands and fingers. She turned her head \n",
            "a hundred times, a\n",
            "\n",
            "[201 | 469.73] loss=2.50 avg=2.76\n",
            "[202 | 471.95] loss=2.51 avg=2.76\n",
            "[203 | 474.17] loss=2.59 avg=2.76\n",
            "[204 | 476.40] loss=2.56 avg=2.76\n",
            "[205 | 478.61] loss=2.66 avg=2.75\n",
            "[206 | 480.83] loss=2.82 avg=2.76\n",
            "[207 | 483.04] loss=2.70 avg=2.75\n",
            "[208 | 485.26] loss=2.52 avg=2.75\n",
            "[209 | 487.48] loss=2.58 avg=2.75\n",
            "[210 | 489.70] loss=2.73 avg=2.75\n",
            "[211 | 491.92] loss=2.40 avg=2.75\n",
            "[212 | 494.14] loss=2.47 avg=2.74\n",
            "[213 | 496.36] loss=2.54 avg=2.74\n",
            "[214 | 498.57] loss=2.58 avg=2.74\n",
            "[215 | 500.79] loss=2.49 avg=2.74\n",
            "[216 | 503.01] loss=2.48 avg=2.73\n",
            "[217 | 505.23] loss=2.46 avg=2.73\n",
            "[218 | 507.45] loss=2.58 avg=2.73\n",
            "[219 | 509.66] loss=2.41 avg=2.72\n",
            "[220 | 511.89] loss=2.79 avg=2.73\n",
            "[221 | 514.12] loss=2.65 avg=2.72\n",
            "[222 | 516.34] loss=2.47 avg=2.72\n",
            "[223 | 518.56] loss=2.41 avg=2.72\n",
            "[224 | 520.78] loss=2.46 avg=2.72\n",
            "[225 | 522.99] loss=2.66 avg=2.71\n",
            "[226 | 525.22] loss=2.70 avg=2.71\n",
            "[227 | 527.45] loss=2.58 avg=2.71\n",
            "[228 | 529.66] loss=2.47 avg=2.71\n",
            "[229 | 531.88] loss=2.30 avg=2.71\n",
            "[230 | 534.10] loss=2.30 avg=2.70\n",
            "[231 | 536.32] loss=2.38 avg=2.70\n",
            "[232 | 538.55] loss=2.66 avg=2.70\n",
            "[233 | 540.77] loss=2.63 avg=2.70\n",
            "[234 | 542.99] loss=2.50 avg=2.69\n",
            "[235 | 545.21] loss=2.44 avg=2.69\n",
            "[236 | 547.43] loss=2.50 avg=2.69\n",
            "[237 | 549.66] loss=2.40 avg=2.69\n",
            "[238 | 551.89] loss=2.51 avg=2.68\n",
            "[239 | 554.11] loss=2.58 avg=2.68\n",
            "[240 | 556.33] loss=2.39 avg=2.68\n",
            "[241 | 558.55] loss=2.37 avg=2.68\n",
            "[242 | 560.77] loss=2.61 avg=2.68\n",
            "[243 | 562.99] loss=2.48 avg=2.67\n",
            "[244 | 565.22] loss=2.54 avg=2.67\n",
            "[245 | 567.44] loss=2.42 avg=2.67\n",
            "[246 | 569.66] loss=2.65 avg=2.67\n",
            "[247 | 571.88] loss=2.45 avg=2.67\n",
            "[248 | 574.11] loss=2.37 avg=2.66\n",
            "[249 | 576.33] loss=2.50 avg=2.66\n",
            "[250 | 578.55] loss=2.37 avg=2.66\n",
            "[251 | 580.78] loss=2.37 avg=2.66\n",
            "[252 | 583.00] loss=2.49 avg=2.65\n",
            "[253 | 585.23] loss=2.44 avg=2.65\n",
            "[254 | 587.45] loss=2.32 avg=2.65\n",
            "[255 | 589.68] loss=2.37 avg=2.64\n",
            "[256 | 591.91] loss=2.52 avg=2.64\n",
            "[257 | 594.13] loss=2.12 avg=2.64\n",
            "[258 | 596.36] loss=2.36 avg=2.63\n",
            "[259 | 598.59] loss=2.38 avg=2.63\n",
            "[260 | 600.81] loss=2.42 avg=2.63\n",
            "[261 | 603.03] loss=2.36 avg=2.63\n",
            "[262 | 605.26] loss=2.37 avg=2.62\n",
            "[263 | 607.48] loss=2.48 avg=2.62\n",
            "[264 | 609.72] loss=2.71 avg=2.62\n",
            "[265 | 611.94] loss=2.42 avg=2.62\n",
            "[266 | 614.16] loss=2.43 avg=2.62\n",
            "[267 | 616.39] loss=2.37 avg=2.62\n",
            "[268 | 618.61] loss=2.39 avg=2.61\n",
            "[269 | 620.83] loss=2.45 avg=2.61\n",
            "[270 | 623.06] loss=2.38 avg=2.61\n",
            "[271 | 625.29] loss=2.43 avg=2.61\n",
            "[272 | 627.51] loss=2.50 avg=2.61\n",
            "[273 | 629.73] loss=2.32 avg=2.60\n",
            "[274 | 631.95] loss=2.47 avg=2.60\n",
            "[275 | 634.19] loss=2.35 avg=2.60\n",
            "[276 | 636.41] loss=2.45 avg=2.60\n",
            "[277 | 638.63] loss=2.27 avg=2.59\n",
            "[278 | 640.86] loss=2.44 avg=2.59\n",
            "[279 | 643.07] loss=2.40 avg=2.59\n",
            "[280 | 645.30] loss=2.16 avg=2.59\n",
            "[281 | 647.53] loss=2.38 avg=2.58\n",
            "[282 | 649.75] loss=2.54 avg=2.58\n",
            "[283 | 651.97] loss=2.50 avg=2.58\n",
            "[284 | 654.19] loss=2.31 avg=2.58\n",
            "[285 | 656.41] loss=2.15 avg=2.58\n",
            "[286 | 658.64] loss=2.24 avg=2.57\n",
            "[287 | 660.86] loss=2.52 avg=2.57\n",
            "[288 | 663.09] loss=2.41 avg=2.57\n",
            "[289 | 665.31] loss=2.41 avg=2.57\n",
            "[290 | 667.53] loss=2.24 avg=2.56\n",
            "[291 | 669.75] loss=2.53 avg=2.56\n",
            "[292 | 671.98] loss=2.24 avg=2.56\n",
            "[293 | 674.20] loss=2.46 avg=2.56\n",
            "[294 | 676.42] loss=2.33 avg=2.56\n",
            "[295 | 678.64] loss=2.42 avg=2.56\n",
            "[296 | 680.86] loss=2.21 avg=2.55\n",
            "[297 | 683.08] loss=2.33 avg=2.55\n",
            "[298 | 685.31] loss=2.36 avg=2.55\n",
            "[299 | 687.53] loss=2.38 avg=2.55\n",
            "[300 | 689.75] loss=2.57 avg=2.55\n",
            "======== SAMPLE 1 ========\n",
            " and \n",
            "knew he might face the same fate. The king's guard would be cut off from all sides.\" \n",
            "Jon Snow smiled. \"If I recall, the Hand for the King in King's Landing has been sending out his guards, \n",
            "to keep \n",
            "Page 561\n",
            "\n",
            "them safe from harm.\" \n",
            "\"Will you not send your son to join the Night's Watch?\" \n",
            "\"Unless you are willing to pay a great price, I fear.\" \n",
            "Jon looked up. \"I know you are, my lord.\" \n",
            "\"Only if we are willing to pay,\" Ned said. If only for an instant. Then he turned his eyes on the rest of the \n",
            "world. \"I will send you as many men as you command as well.\" \n",
            "\"They will not serve the king, and the boy will not be a knight if his father dies,\" Ned promised. \"The king \n",
            "is still King Robert. He was my father's Hand in the East.\" \n",
            "Not long ago, that was Edmure Tallhart, the young commander of the guard sent to guard the Kingsguard in \n",
            "Jaime Lannister's stead. The boy had been sent as a third lance, but even so he had not been \n",
            "as powerful as the likes of Ned and Dickon had dreamed; in the armory he'd found an extra .38-caliber \n",
            "blade he'd borrowed, and there it was, a half-dozen guardsmen waiting in a tall, oak-lined room in the \n",
            "great hall below Winterfell, looking for the king. \n",
            "\"What we do know is that the king does not like his guardsmen much, but he likes to play the game of \n",
            "Kings at our court,\" said Will Romm, an older man with a bald head, \n",
            "and an ancient black beard. \"So Ned sent him, and for three days he played the game to the king. \n",
            "Page 560\n",
            "\n",
            "The king loved him, Ned thought stiffly. But Ned had to agree that the king might not take part. \n",
            "\"It was the king's thing, right? When the gods gave him the throne, he gave the king's stuff.\" \n",
            "That was the easy part. The hard part. Jon Snow had been dead three years, and the Lannisters were \n",
            "just four of his liege lords. He had loved him so much now, in truth, that his father had feared that \n",
            "some day he would be murdered. \n",
            "\"The question is,\" Jory declared solemnly. The man whose office that day must have \n",
            "been assigned was none other than Eddard Stark himself, the younger brother of the king. He was clad in a \n",
            "banner that read, \"No, The Sword of the Morning, and if I may be so bold as to say that I must call \n",
            "him a coward, and make no pretended vow to any one who would listen, for he has the blade \n",
            "for himself, and that he will do well to get it.\" He looked almost as though he might \n",
            "have an answer for the boy who had made him swear that he had no right to it, nor for his younger brother \n",
            "Edmure. He was not a coward, he thought with a sense of certainty. But he had no real right to the \n",
            "sword. \n",
            "As he sat cross-legged in the shadow of the broken sword, Jon Snow felt sick to his stomach. \n",
            "\"What a pity your brother is dead,\" R.I., muttered Rasto. \"He wouldn't have told any one, would \n",
            "he? All the time. You didn't even care to know how he killed him?\" \n",
            "\"There were swords for Robb the Robb the Robb, for Robb.\" \n",
            "\"All the time,\" said Rasto. \"I could feel the blood running down his face. He could not have said who he \n",
            "was fighting, but there \n",
            "were swords for Robb too, to see who they were. He wanted to kill Robb, so he took the sword and the \n",
            "blood, and all the time there were no more swords. You could see it . . . he was a good Stark, he \n",
            "said the words. They are a song for Robb's ears, the words . . . the songs. Robb the Robb, \n",
            "the music said. \n",
            "\"You are hurting my son, \" Jon had his voice down twisted but it was true. \"You are hurting \n",
            "my son, \" he shouted back at him, \"I won, I beat your father, I gave the boy the sword. So what \n",
            "is it that we call the boy? That is who he was once, only a boy . . . a bastard?\" \n",
            "He glanced sideways. He knew how the king could feel, that was all, the thought made him sick. \"They\n",
            "\n",
            "[301 | 703.11] loss=2.72 avg=2.55\n",
            "[302 | 705.33] loss=2.28 avg=2.54\n",
            "[303 | 707.56] loss=2.29 avg=2.54\n",
            "[304 | 709.79] loss=2.33 avg=2.54\n",
            "[305 | 712.01] loss=2.25 avg=2.54\n",
            "[306 | 714.23] loss=2.14 avg=2.53\n",
            "[307 | 716.45] loss=2.25 avg=2.53\n",
            "[308 | 718.68] loss=2.25 avg=2.53\n",
            "[309 | 720.91] loss=2.30 avg=2.52\n",
            "[310 | 723.12] loss=2.31 avg=2.52\n",
            "[311 | 725.34] loss=2.10 avg=2.52\n",
            "[312 | 727.57] loss=2.28 avg=2.52\n",
            "[313 | 729.78] loss=2.29 avg=2.51\n",
            "[314 | 732.01] loss=2.38 avg=2.51\n",
            "[315 | 734.23] loss=2.32 avg=2.51\n",
            "[316 | 736.45] loss=2.21 avg=2.51\n",
            "[317 | 738.67] loss=2.14 avg=2.50\n",
            "[318 | 740.90] loss=2.31 avg=2.50\n",
            "[319 | 743.12] loss=2.12 avg=2.50\n",
            "[320 | 745.34] loss=2.27 avg=2.49\n",
            "[321 | 747.56] loss=2.40 avg=2.49\n",
            "[322 | 749.79] loss=2.30 avg=2.49\n",
            "[323 | 752.01] loss=2.41 avg=2.49\n",
            "[324 | 754.24] loss=2.16 avg=2.49\n",
            "[325 | 756.47] loss=2.31 avg=2.49\n",
            "[326 | 758.69] loss=2.32 avg=2.48\n",
            "[327 | 760.91] loss=2.20 avg=2.48\n",
            "[328 | 763.13] loss=2.41 avg=2.48\n",
            "[329 | 765.36] loss=2.08 avg=2.48\n",
            "[330 | 767.58] loss=2.25 avg=2.47\n",
            "[331 | 769.80] loss=2.34 avg=2.47\n",
            "[332 | 772.03] loss=2.14 avg=2.47\n",
            "[333 | 774.25] loss=2.17 avg=2.47\n",
            "[334 | 776.47] loss=2.41 avg=2.46\n",
            "[335 | 778.70] loss=2.21 avg=2.46\n",
            "[336 | 780.92] loss=2.44 avg=2.46\n",
            "[337 | 783.15] loss=2.25 avg=2.46\n",
            "[338 | 785.37] loss=2.11 avg=2.46\n",
            "[339 | 787.59] loss=2.21 avg=2.45\n",
            "[340 | 789.81] loss=2.34 avg=2.45\n",
            "[341 | 792.04] loss=2.21 avg=2.45\n",
            "[342 | 794.26] loss=2.32 avg=2.45\n",
            "[343 | 796.48] loss=2.02 avg=2.44\n",
            "[344 | 798.71] loss=2.30 avg=2.44\n",
            "[345 | 800.93] loss=2.17 avg=2.44\n",
            "[346 | 803.16] loss=2.03 avg=2.44\n",
            "[347 | 805.39] loss=2.11 avg=2.43\n",
            "[348 | 807.62] loss=2.14 avg=2.43\n",
            "[349 | 809.84] loss=2.25 avg=2.43\n",
            "[350 | 812.07] loss=2.04 avg=2.42\n",
            "[351 | 814.29] loss=2.26 avg=2.42\n",
            "[352 | 816.51] loss=2.12 avg=2.42\n",
            "[353 | 818.74] loss=2.21 avg=2.42\n",
            "[354 | 820.96] loss=2.12 avg=2.41\n",
            "[355 | 823.19] loss=2.32 avg=2.41\n",
            "[356 | 825.41] loss=2.10 avg=2.41\n",
            "[357 | 827.64] loss=2.24 avg=2.41\n",
            "[358 | 829.86] loss=2.19 avg=2.41\n",
            "[359 | 832.08] loss=2.21 avg=2.40\n",
            "[360 | 834.30] loss=1.99 avg=2.40\n",
            "[361 | 836.52] loss=2.53 avg=2.40\n",
            "[362 | 838.74] loss=2.17 avg=2.40\n",
            "[363 | 840.98] loss=2.21 avg=2.40\n",
            "[364 | 843.20] loss=2.07 avg=2.39\n",
            "[365 | 845.43] loss=2.26 avg=2.39\n",
            "[366 | 847.65] loss=2.01 avg=2.39\n",
            "[367 | 849.87] loss=2.29 avg=2.39\n",
            "[368 | 852.09] loss=2.13 avg=2.38\n",
            "[369 | 854.31] loss=1.99 avg=2.38\n",
            "[370 | 856.53] loss=2.06 avg=2.38\n",
            "[371 | 858.75] loss=2.15 avg=2.37\n",
            "[372 | 860.97] loss=2.43 avg=2.37\n",
            "[373 | 863.19] loss=2.28 avg=2.37\n",
            "[374 | 865.42] loss=2.17 avg=2.37\n",
            "[375 | 867.64] loss=1.97 avg=2.37\n",
            "[376 | 869.86] loss=2.33 avg=2.37\n",
            "[377 | 872.08] loss=2.15 avg=2.37\n",
            "[378 | 874.30] loss=2.15 avg=2.36\n",
            "[379 | 876.53] loss=2.02 avg=2.36\n",
            "[380 | 878.75] loss=2.06 avg=2.36\n",
            "[381 | 880.98] loss=1.70 avg=2.35\n",
            "[382 | 883.20] loss=2.22 avg=2.35\n",
            "[383 | 885.42] loss=2.20 avg=2.35\n",
            "[384 | 887.64] loss=2.26 avg=2.35\n",
            "[385 | 889.87] loss=2.08 avg=2.34\n",
            "[386 | 892.09] loss=1.99 avg=2.34\n",
            "[387 | 894.31] loss=2.25 avg=2.34\n",
            "[388 | 896.53] loss=1.80 avg=2.33\n",
            "[389 | 898.75] loss=2.16 avg=2.33\n",
            "[390 | 900.98] loss=2.32 avg=2.33\n",
            "[391 | 903.21] loss=1.94 avg=2.33\n",
            "[392 | 905.43] loss=2.02 avg=2.32\n",
            "[393 | 907.65] loss=2.24 avg=2.32\n",
            "[394 | 909.87] loss=2.32 avg=2.32\n",
            "[395 | 912.08] loss=1.92 avg=2.32\n",
            "[396 | 914.31] loss=2.18 avg=2.32\n",
            "[397 | 916.53] loss=1.70 avg=2.31\n",
            "[398 | 918.75] loss=1.98 avg=2.31\n",
            "[399 | 920.97] loss=2.06 avg=2.31\n",
            "[400 | 923.19] loss=1.84 avg=2.30\n",
            "======== SAMPLE 1 ========\n",
            " only so much as an innkeep's hideout. Her brother was out prowling, all blood and maimed and the \n",
            "lord scrambling for cover. \"No one should be allowed to steal the bride,\" Stark said, \"so long as you are accompanied by \n",
            "a boy.\" \n",
            "The word got around the camp like a storm, and there was scarcely anyone left but Jon and the boy in \n",
            "the woods, even after the direwolves had gone dark again and the black wolf had gained ground on the \n",
            "two of them. A few ragged men had taken to riding on horses of that size when some wildling \n",
            "slashed them by the wrist between the ears and ended their lives. \n",
            "A GAML OF THRONLS 109 \n",
            "Jon Snow's eyes darted from the direwolf to the direwolf again and back, then he turned his \n",
            "fren to the direwolf and found that it was the other one that saw him cut. Three men were all taken, he \n",
            "said. A few of the men were with him, with horses as well. Ser Rodrik and Cohollo had been fighting near the \n",
            "leaden hours, while Jon and his friends went to the Shadow Tower to train. \n",
            "A GAML OF THRONLS 110 \n",
            "\"I have a dream,\" the raven warned. \"I'm with the girl . . .\" \n",
            "Jon looked to his brother. \"You and Yoren, that is not dream-\" \n",
            "\"We won the coin,\" Jon swore. \"We should have won the coin!\" \n",
            "Robb went to fetch it. \"What do you want with the coin, bastard?\" \n",
            "\"Give Ben my gold and I'll go get him,\" Jon snapped. \"As you command.\" \n",
            "Page 116\n",
            "\n",
            "Robb was very anxious to have his gold. The gold was a token of his pride and loyalty, if not \n",
            "desertion. \n",
            "That had not stopped Jon. He gave his brother the coin for some token of his pride and \n",
            "bravado, and Jon looked at it gravely. It was the color of sunflower and satin. \"Now give me my \n",
            "gold and I'll leave you,\" he told them all. \n",
            "Jon looked at the coin. It had more gold than Ben Stark, but not as much as the man with the golden \n",
            "coats. The Lannisters were well-known for their coinage. \n",
            "\"The gold they give me says is their coin.\" \n",
            "\"The Lannisters are the coin kings,\" Jon said. \"I don't want Ben Stark of Winterfell to have his \n",
            "gold.\" \n",
            "There was laughter in the Common Tongue. The men outside the camp exchanged glances, but nothing came of it \n",
            "until it had gone south. \"I don't want Jon Snow to have his gold,\" Jon said with a resigned look of contempt. \n",
            "That was the end of him. He took a knife and threatened to leave the camps anytime soon. He had \n",
            "no intention of leading the Lannisters back to their former ways. At least not without Jon's help, and the \n",
            "Lannister commons had taken a good interest in the matter. The Stark children were still wed, and his oldest, Jon \n",
            "Stark, had been named. \n",
            "The men outside the camp exchanged glances, but nothing came of it until it had gone south. Jon Stark \n",
            "had taken another knife. He was calling the shots. \"Help is at hand!\" the innkeep said when \n",
            "Jon Snow rose to leave the tent. \"Leave me. I will not see you here.\" He bowed his head, his head bowed. His \n",
            "cloak had turned a pale blue as the moon. \n",
            "It was a terrible cold morning in the camp, and Jon Snow walked his men through the trees in a \n",
            "mummer's manner. It was warm in the cool morning air, but he seemed to get the better of most \n",
            "of the men. He was wearing a light blue woolen cloak, heavy with the golden ring of the Hall of \n",
            "House. Jon knelt, took a deep breath to close it, and looked at Lord Eddard with sudden concern. \n",
            "\"As you wish, Jon Snow. If you would be so kind, give my father your pardons and leave him to deal with our \n",
            "contrarian . . .\" \n",
            "The man with the golden chain returned Jon's call. \"Tell him,\" he said. \n",
            "Jon looked down and saw that the gold-cloaked handmaid had vanished in a heartbeat. Her hand was bare, \n",
            "and she was dressed in nothing but scraps of boiled silk and dried leaves. If he were truly a traitor, he \n",
            "might well be sent to a dungeon somewhere, but he had no choice but to go along with the Lannisters for \n",
            "sure . .\n",
            "\n",
            "[401 | 936.63] loss=1.92 avg=2.30\n",
            "[402 | 938.85] loss=2.11 avg=2.30\n",
            "[403 | 941.07] loss=2.08 avg=2.29\n",
            "[404 | 943.29] loss=2.17 avg=2.29\n",
            "[405 | 945.50] loss=2.33 avg=2.29\n",
            "[406 | 947.72] loss=2.18 avg=2.29\n",
            "[407 | 949.96] loss=1.89 avg=2.29\n",
            "[408 | 952.18] loss=2.28 avg=2.29\n",
            "[409 | 954.40] loss=1.95 avg=2.28\n",
            "[410 | 956.62] loss=2.11 avg=2.28\n",
            "[411 | 958.84] loss=1.83 avg=2.28\n",
            "[412 | 961.06] loss=1.77 avg=2.27\n",
            "[413 | 963.29] loss=2.17 avg=2.27\n",
            "[414 | 965.51] loss=1.98 avg=2.27\n",
            "[415 | 967.73] loss=1.82 avg=2.26\n",
            "[416 | 969.95] loss=1.96 avg=2.26\n",
            "[417 | 972.18] loss=2.08 avg=2.26\n",
            "[418 | 974.41] loss=1.97 avg=2.26\n",
            "[419 | 976.63] loss=1.88 avg=2.25\n",
            "[420 | 978.86] loss=1.98 avg=2.25\n",
            "[421 | 981.08] loss=2.21 avg=2.25\n",
            "[422 | 983.31] loss=1.95 avg=2.25\n",
            "[423 | 985.53] loss=1.93 avg=2.24\n",
            "[424 | 987.76] loss=2.15 avg=2.24\n",
            "[425 | 989.98] loss=2.14 avg=2.24\n",
            "[426 | 992.21] loss=1.91 avg=2.24\n",
            "[427 | 994.44] loss=1.84 avg=2.23\n",
            "[428 | 996.66] loss=1.93 avg=2.23\n",
            "[429 | 998.90] loss=2.09 avg=2.23\n",
            "[430 | 1001.13] loss=2.22 avg=2.23\n",
            "[431 | 1003.35] loss=1.53 avg=2.22\n",
            "[432 | 1005.58] loss=2.12 avg=2.22\n",
            "[433 | 1007.81] loss=2.05 avg=2.22\n",
            "[434 | 1010.04] loss=1.90 avg=2.22\n",
            "[435 | 1012.27] loss=1.88 avg=2.21\n",
            "[436 | 1014.50] loss=1.99 avg=2.21\n",
            "[437 | 1016.72] loss=2.11 avg=2.21\n",
            "[438 | 1018.94] loss=2.09 avg=2.21\n",
            "[439 | 1021.17] loss=1.85 avg=2.20\n",
            "[440 | 1023.41] loss=1.94 avg=2.20\n",
            "[441 | 1025.64] loss=2.00 avg=2.20\n",
            "[442 | 1027.87] loss=2.03 avg=2.20\n",
            "[443 | 1030.09] loss=2.00 avg=2.20\n",
            "[444 | 1032.32] loss=2.17 avg=2.20\n",
            "[445 | 1034.54] loss=1.91 avg=2.19\n",
            "[446 | 1036.77] loss=1.86 avg=2.19\n",
            "[447 | 1038.99] loss=1.91 avg=2.19\n",
            "[448 | 1041.22] loss=1.66 avg=2.18\n",
            "[449 | 1043.44] loss=1.75 avg=2.18\n",
            "[450 | 1045.67] loss=2.19 avg=2.18\n",
            "[451 | 1047.91] loss=1.68 avg=2.17\n",
            "[452 | 1050.13] loss=1.85 avg=2.17\n",
            "[453 | 1052.36] loss=1.97 avg=2.17\n",
            "[454 | 1054.58] loss=1.77 avg=2.16\n",
            "[455 | 1056.80] loss=1.79 avg=2.16\n",
            "[456 | 1059.03] loss=1.99 avg=2.16\n",
            "[457 | 1061.26] loss=2.01 avg=2.16\n",
            "[458 | 1063.48] loss=1.90 avg=2.15\n",
            "[459 | 1065.70] loss=1.94 avg=2.15\n",
            "[460 | 1067.92] loss=2.05 avg=2.15\n",
            "[461 | 1070.14] loss=1.76 avg=2.15\n",
            "[462 | 1072.38] loss=1.87 avg=2.14\n",
            "[463 | 1074.60] loss=1.70 avg=2.14\n",
            "[464 | 1076.82] loss=1.79 avg=2.13\n",
            "[465 | 1079.04] loss=1.68 avg=2.13\n",
            "[466 | 1081.26] loss=1.77 avg=2.13\n",
            "[467 | 1083.48] loss=2.03 avg=2.13\n",
            "[468 | 1085.71] loss=1.99 avg=2.12\n",
            "[469 | 1087.94] loss=1.74 avg=2.12\n",
            "[470 | 1090.16] loss=1.92 avg=2.12\n",
            "[471 | 1092.38] loss=1.83 avg=2.12\n",
            "[472 | 1094.61] loss=1.74 avg=2.11\n",
            "[473 | 1096.84] loss=1.74 avg=2.11\n",
            "[474 | 1099.06] loss=1.92 avg=2.11\n",
            "[475 | 1101.28] loss=1.81 avg=2.10\n",
            "[476 | 1103.50] loss=2.24 avg=2.10\n",
            "[477 | 1105.73] loss=1.79 avg=2.10\n",
            "[478 | 1107.95] loss=1.77 avg=2.10\n",
            "[479 | 1110.18] loss=1.75 avg=2.09\n",
            "[480 | 1112.39] loss=1.98 avg=2.09\n",
            "[481 | 1114.62] loss=1.90 avg=2.09\n",
            "[482 | 1116.84] loss=1.99 avg=2.09\n",
            "[483 | 1119.06] loss=1.76 avg=2.09\n",
            "[484 | 1121.29] loss=1.81 avg=2.08\n",
            "[485 | 1123.51] loss=1.96 avg=2.08\n",
            "[486 | 1125.73] loss=1.61 avg=2.08\n",
            "[487 | 1127.95] loss=1.92 avg=2.08\n",
            "[488 | 1130.17] loss=1.92 avg=2.08\n",
            "[489 | 1132.40] loss=1.97 avg=2.07\n",
            "[490 | 1134.62] loss=1.81 avg=2.07\n",
            "[491 | 1136.84] loss=1.66 avg=2.07\n",
            "[492 | 1139.06] loss=1.88 avg=2.07\n",
            "[493 | 1141.28] loss=1.81 avg=2.06\n",
            "[494 | 1143.50] loss=1.83 avg=2.06\n",
            "[495 | 1145.73] loss=2.00 avg=2.06\n",
            "[496 | 1147.95] loss=2.10 avg=2.06\n",
            "[497 | 1150.17] loss=1.76 avg=2.06\n",
            "[498 | 1152.39] loss=1.71 avg=2.05\n",
            "[499 | 1154.61] loss=1.82 avg=2.05\n",
            "[500 | 1156.83] loss=1.56 avg=2.05\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919cb3c6-3a52-41ab-b9da-94ac0fa1f0ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth? \n",
            "No, she thought. The thought made her heart jump. The moon was so far away. She mustn't even be afraid. \n",
            "She would die. She would go, and the world would be a better place. \n",
            "The moon was so low she could see only a few hundred feet below her. It would be too dark to see the \n",
            "flat earth that sheltered the earth throne. She would have to go, and the world might be too dark to see the \n",
            "solar bears circling the Wall. She would have to go, and the stars would be black and dreadful. \n",
            "She would have to go, and the gods would say the world was a thousand miles long and vast and beautiful. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "564ebb74-2ba5-403f-dc4b-c36ce1478d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7K9X3K8TEwj",
        "outputId": "d0760c42-a0e4-4dcf-b7cc-ca98aaffa2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:49:16--  https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329071 (321K) [text/plain]\n",
            "Saving to: â€˜pg1597.txtâ€™\n",
            "\n",
            "pg1597.txt          100%[===================>] 321.36K   800KB/s    in 0.4s    \n",
            "\n",
            "2023-03-21 14:49:22 (800 KB/s) - â€˜pg1597.txtâ€™ saved [329071/329071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf360b-ce90-4a36-d434-44820124b877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:25:10--  https://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807231 (788K) [text/plain]\n",
            "Saving to: â€˜98-0.txtâ€™\n",
            "\n",
            "98-0.txt            100%[===================>] 788.31K   718KB/s    in 1.1s    \n",
            "\n",
            "2023-02-22 13:25:12 (718 KB/s) - â€˜98-0.txtâ€™ saved [807231/807231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}