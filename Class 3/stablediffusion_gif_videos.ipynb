{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5QG2HNwdcb-"
      },
      "source": [
        "Source: [Creating AI Music Videos with Stable Diffusion\n",
        "description: A tutorial on how to create AI music videos using the text-to-image model Stable Diffusion., Nate Raw, 2022-10-11](https://colab.research.google.com/github/nateraw/aiart-blog/blob/main/posts/sd-music-videos/sd_music_videos.ipynb#scrollTo=9RrN7cNLW17w)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/stablediffusion_gif_videos.ipynb#scrollTo=NBQ7BqoPVcZS)"
      ],
      "metadata": {
        "id": "NBQ7BqoPVcZS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_siKclWNzUN"
      },
      "source": [
        "# Creating Short Videos (gifs) with Stable Diffusion\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Theoretical introduction"
      ],
      "metadata": {
        "id": "Mu-2yBolVQIJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgzpMOvVIBoG"
      },
      "source": [
        "\n",
        "\n",
        "Today, we'll talk about how we can leverage Stable Diffusion to generate captivating music videos that move to the beat of a song."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQbyQfdfdccD"
      },
      "source": [
        "###Examples\n",
        "\n",
        "Here are some examples of videos you'll be able to generate using the techniques we describe below:\n",
        "\n",
        "  - [Example 1](https://twitter.com/_nateraw/status/1578049976206237705)\n",
        "  - [Example 2](https://twitter.com/_nateraw/status/1578056368078086144)\n",
        "  - [Example 3](https://twitter.com/_nateraw/status/1580322593240133632)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nRfWjeYOyma"
      },
      "source": [
        "### Stable Diffusion Inference\n",
        "\n",
        "At the most basic level, all you need to know is that the model we'll be working with takes in 2 inputs:\n",
        "\n",
        "1. A random noise vector (containing samples drawn from a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution))\n",
        "2. A text prompt that will be used to condition the image generation diffusion process\n",
        "\n",
        "All else remaining constant, if you pass the *same* noise vector to the model more than once, it will generate the **exact same image**.\n",
        "\n",
        "However, **if you change that noise vector** a different collection of samples drawn from a normal distribution, **you'll get a different image**.\n",
        "\n",
        "For a more in depth explanation of how diffusion models work, check out this [legendary writeup on the Hugging Face blog](https://huggingface.co/blog/annotated-diffusion)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6KiNWiRJMd6"
      },
      "source": [
        "### Interpolating the Latent Space\n",
        "\n",
        "If we slowly interpolate between two random noise vectors and generate images along the way, we should be able to \"dream between\" the images generated by the two vectors.\n",
        "\n",
        "Representing the noise vectors as \"Image A\" and \"Image B\", this is more or less what we'll do first:\n",
        "\n",
        "![interpolation img](https://huggingface.co/datasets/nateraw/documentation-images/resolve/main/linear-interpolation.jpeg)\n",
        "\n",
        "\"Interpolating\" here basically means mixing between Image A and Image B. At step 0, we have Image A's noise vector. At the last step, we have Image B's noise vector. \n",
        "\n",
        "As we step through, the % mix of Image A's noise vector decreases, and % mix of Image B's noise vector increases.\n",
        "\n",
        "We'll use [Spherical Linear Interpolation](https://en.wikipedia.org/wiki/Slerp) to generate vectors in between `A` and `B` at weights `t`, as it seems to work best for these random noise vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's install the necessary libraries"
      ],
      "metadata": {
        "id": "2hGNPht-U7bv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky5D9nDZMio6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install diffusers==0.4.0 transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5IASM8qmHHN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7XxFw3fPVCl"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "\n",
        "#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"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToYA3kidOsD6"
      },
      "source": [
        "### Implementation with Diffusers\n",
        "\n",
        "We'll use Hugging Face's [`diffusers`](https://github.com/huggingface/diffusers) to download and interface with Stable Diffusion.\n",
        "\n",
        "In that codebase, there's a `StableDiffusionPipeline`, which is a handy wrapper for inference."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade diffusers transformers scipy"
      ],
      "metadata": {
        "id": "wUY2QqHBjwVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate"
      ],
      "metadata": {
        "id": "TNgWTXlkkOXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline"
      ],
      "metadata": {
        "id": "1f_n1Xj4GSaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##And create functions we need"
      ],
      "metadata": {
        "id": "VPioWlhHZaNE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot3ojbz4N6va"
      },
      "outputs": [],
      "source": [
        "def slerp(t, v0, v1, DOT_THRESHOLD=0.9995):\n",
        "    \"\"\"helper function to spherically interpolate two arrays v1 v2\"\"\"\n",
        "    if not isinstance(v0, np.ndarray):\n",
        "        inputs_are_torch = True\n",
        "        input_device = v0.device\n",
        "        v0 = v0.cpu().numpy()\n",
        "        v1 = v1.cpu().numpy()\n",
        "    dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))\n",
        "\n",
        "    if np.abs(dot) > DOT_THRESHOLD:\n",
        "        v2 = (1 - t) * v0 + t * v1\n",
        "    else:\n",
        "        theta_0 = np.arccos(dot)\n",
        "        sin_theta_0 = np.sin(theta_0)\n",
        "        theta_t = theta_0 * t\n",
        "        sin_theta_t = np.sin(theta_t)\n",
        "        s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
        "        s1 = sin_theta_t / sin_theta_0\n",
        "        v2 = s0 * v0 + s1 * v1\n",
        "\n",
        "    if inputs_are_torch:\n",
        "        v2 = torch.from_numpy(v2).to(input_device)\n",
        "    return v2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's create an image"
      ],
      "metadata": {
        "id": "JcnumQX1Z03z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "device = \"cuda\"\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "prompt = \"cat and cake\"\n",
        "image = pipe(prompt).images[0]  \n",
        "    \n",
        "#image.save(\"image1.png\")"
      ],
      "metadata": {
        "id": "j3MltugsjZmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "G-BUZ83WkZUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0oNJG8odccM"
      },
      "source": [
        "Here's the full code for our `StableDiffusionWalkPipeline`, the piece of code we need to create a gif."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHQA4d53bFD_"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "from typing import Optional, Union, List, Callable\n",
        "\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
        "\n",
        "class StableDiffusionWalkPipeline(StableDiffusionPipeline):\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Optional[Union[str, List[str]]] = None,\n",
        "        height: int = 512,\n",
        "        width: int = 512,\n",
        "        num_inference_steps: int = 50,\n",
        "        guidance_scale: float = 7.5,\n",
        "        eta: float = 0.0,\n",
        "        generator: Optional[torch.Generator] = None,\n",
        "        latents: Optional[torch.FloatTensor] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        return_dict: bool = True,\n",
        "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
        "        callback_steps: Optional[int] = 1,\n",
        "        text_embeddings: Optional[torch.FloatTensor] = None,\n",
        "    ):\n",
        "\n",
        "        if height % 8 != 0 or width % 8 != 0:\n",
        "            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "        if (callback_steps is None) or (\n",
        "            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n",
        "                f\" {type(callback_steps)}.\"\n",
        "            )\n",
        "\n",
        "        if text_embeddings is None:\n",
        "            if isinstance(prompt, str):\n",
        "                batch_size = 1\n",
        "            elif isinstance(prompt, list):\n",
        "                batch_size = len(prompt)\n",
        "            else:\n",
        "                raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "            # get prompt text embeddings\n",
        "            text_inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            text_input_ids = text_inputs.input_ids\n",
        "\n",
        "            if text_input_ids.shape[-1] > self.tokenizer.model_max_length:\n",
        "                removed_text = self.tokenizer.batch_decode(text_input_ids[:, self.tokenizer.model_max_length :])\n",
        "                print(\n",
        "                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
        "                    f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n",
        "                )\n",
        "                text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n",
        "            text_embeddings = self.text_encoder(text_input_ids.to(self.device))[0]\n",
        "        else:\n",
        "            batch_size = text_embeddings.shape[0]\n",
        "\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        if do_classifier_free_guidance:\n",
        "            # HACK - Not setting text_input_ids here when walking, so hard coding to max length of tokenizer\n",
        "            # TODO - Determine if this is OK to do\n",
        "            # max_length = text_input_ids.shape[-1]\n",
        "            max_length = self.tokenizer.model_max_length\n",
        "            uncond_input = self.tokenizer(\n",
        "                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "            )\n",
        "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "        # get the initial random noise unless the user supplied it\n",
        "\n",
        "        # Unlike in other pipelines, latents need to be generated in the target device\n",
        "        # for 1-to-1 results reproducibility with the CompVis implementation.\n",
        "        # However this currently doesn't work in `mps`.\n",
        "        latents_device = \"cpu\" if self.device.type == \"mps\" else self.device\n",
        "        latents_shape = (batch_size, self.unet.in_channels, height // 8, width // 8)\n",
        "        if latents is None:\n",
        "            latents = torch.randn(\n",
        "                latents_shape,\n",
        "                generator=generator,\n",
        "                device=latents_device,\n",
        "                dtype=text_embeddings.dtype,\n",
        "            )\n",
        "        else:\n",
        "            if latents.shape != latents_shape:\n",
        "                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {latents_shape}\")\n",
        "            latents = latents.to(latents_device)\n",
        "\n",
        "        # set timesteps\n",
        "        self.scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "        # Some schedulers like PNDM have timesteps as arrays\n",
        "        # It's more optimized to move all timesteps to correct device beforehand\n",
        "        timesteps_tensor = self.scheduler.timesteps.to(self.device)\n",
        "\n",
        "        # scale the initial noise by the standard deviation required by the scheduler\n",
        "        latents = latents * self.scheduler.init_noise_sigma\n",
        "\n",
        "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "        # and should be between [0, 1]\n",
        "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "        extra_step_kwargs = {}\n",
        "        if accepts_eta:\n",
        "            extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "        for i, t in enumerate(self.progress_bar(timesteps_tensor)):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "            # predict the noise residual\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "            # perform guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
        "\n",
        "            # call the callback, if provided\n",
        "            if callback is not None and i % callback_steps == 0:\n",
        "                callback(i, t, latents)\n",
        "\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = self.vae.decode(latents).sample\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "        safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
        "        image, has_nsfw_concept = self.safety_checker(\n",
        "            images=image, clip_input=safety_checker_input.pixel_values.to(text_embeddings.dtype)\n",
        "        )\n",
        "\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        if not return_dict:\n",
        "            return (image, has_nsfw_concept)\n",
        "\n",
        "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enDaQEPaeYE-"
      },
      "source": [
        "Remove existing pipeline instance before proceeding..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TTJTxxed36J"
      },
      "outputs": [],
      "source": [
        "del pipe\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EhYGDFaeg9j"
      },
      "source": [
        "Then, initialize the pipeline just as we did before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uypEJa-leauw"
      },
      "outputs": [],
      "source": [
        "pipeline = StableDiffusionWalkPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\",\n",
        "    revision='fp16'\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VTN713Meo-1"
      },
      "source": [
        "##We need two more functions\n",
        "\n",
        "We'll start by creating two helper functions, `embed_text` and `get_noise` so this repetitive code doesn't muddy up our logic below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmNq8yARfh2R"
      },
      "outputs": [],
      "source": [
        "def embed_text(pipeline, text):\n",
        "    \"\"\"takes in text and turns it into text embeddings\"\"\"\n",
        "    text_input = pipeline.tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        max_length=pipeline.tokenizer.model_max_length,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        embed = pipeline.text_encoder(text_input.input_ids.to(pipeline.device))[0]\n",
        "    return embed\n",
        "\n",
        "def get_noise(pipeline, seed, height=512, width=512):\n",
        "    \"\"\"Takes in random seed and returns corresponding noise vector\"\"\"\n",
        "    return torch.randn(\n",
        "        (1, pipeline.unet.in_channels, height // 8, width // 8),\n",
        "        generator=torch.Generator(\n",
        "            device=pipeline.device\n",
        "        ).manual_seed(seed),\n",
        "        device=pipeline.device,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define prompts for your gif"
      ],
      "metadata": {
        "id": "rO_-QVctbNFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to provide initial values to our images."
      ],
      "metadata": {
        "id": "NyUJOgo5axDp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugFuLOS9g0-l"
      },
      "outputs": [],
      "source": [
        "# Height and width of image are important for noise vector creation\n",
        "# Values should be divisible by 8 if less than 512\n",
        "# Values should be divisible by 64 if greater than 512\n",
        "height, width = 512, 512\n",
        "\n",
        "# Prompts/random seeds for A and B\n",
        "prompt_a, prompt_b = 'profile of a realistic pink horse at a blue sky', 'profile of a realistic rainbow unicorn at a blue sky'\n",
        "seed_a, seed_b = 42, 1337\n",
        "\n",
        "# Noise for A and B\n",
        "noise_a = get_noise(pipeline, seed_a, height=height, width=width)\n",
        "noise_b = get_noise(pipeline, seed_b, height=height, width=width)\n",
        "\n",
        "# Text embeddings for A and B\n",
        "embed_a = embed_text(pipeline, prompt_a)\n",
        "embed_b = embed_text(pipeline, prompt_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate images"
      ],
      "metadata": {
        "id": "5z7g3edxbT4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below to generate images"
      ],
      "metadata": {
        "id": "iG4nxQ_Sa3zK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZXtkgX5hFb8"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "output_dir = Path('images_walk_with_text')\n",
        "output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Steps to interpolate (i.e. number of images to generate)\n",
        "num_interpolation_steps = 10\n",
        "T = torch.linspace(0.0, 1.0, num_interpolation_steps).to(pipeline.device)\n",
        "\n",
        "images = []\n",
        "for i, t in enumerate(T):\n",
        "    noise_t = slerp(float(t), noise_a, noise_b)\n",
        "    embed_t = torch.lerp(embed_a, embed_b, t)\n",
        "    im = pipeline(\n",
        "        text_embeddings=embed_t,\n",
        "        latents=noise_t,\n",
        "        height=height,\n",
        "        width=width\n",
        "    ).images[0]\n",
        "    im.save(output_dir / f'frame{i:06d}.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Merge images as a .gif file"
      ],
      "metadata": {
        "id": "4vHtkfDubYtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use generated images to make a gif."
      ],
      "metadata": {
        "id": "FWJlmFFAa9Co"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCp9_ikbiYXr"
      },
      "outputs": [],
      "source": [
        "! ffmpeg -f image2 -framerate 5 -i images_walk_with_text/frame%06d.png -loop 0 out.gif"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to .mp4 if you like."
      ],
      "metadata": {
        "id": "lhjIAdDmbCFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i out.gif -movflags faststart -pix_fmt yuv420p -vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" video.mp4"
      ],
      "metadata": {
        "id": "Y5HEL-7sUBu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's see the result"
      ],
      "metadata": {
        "id": "ZJEeaP92bgRB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display your gif running code below"
      ],
      "metadata": {
        "id": "c9KXNAA9bHco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "#Image(url='...')  # external\n",
        "fname = '/content/out.gif'\n",
        "Image(open(fname, 'rb').read())  # local"
      ],
      "metadata": {
        "id": "AAye4n6VTEXb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}